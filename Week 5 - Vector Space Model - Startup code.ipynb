{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Vector Space Model (VSM) and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the next weeks, we are going to re-implement Sherin's algorithm and apply it to the text data we've been working on last week! Here's our roadmap:\n",
    "\n",
    "**Week 5 - data cleaning**\n",
    "1. import the data\n",
    "2. clean the data (e.g., remopve stop words, punctuation, etc.)\n",
    "3. build a vocabulary for the dataset\n",
    "4. create chunks of 100 words, with a 25-words overlap\n",
    "5. create a word count matrix, where each chunk of a row and each column represents a word\n",
    "\n",
    "**Week 6 - vectorization and linear algebra**\n",
    "6. Dampen: weight the frequency of words (1 + log[count])\n",
    "7. Scale: Normalize weighted frequency of words\n",
    "8. Direction: compute deviation vectors\n",
    "\n",
    "**Week 7 - Clustering**\n",
    "9. apply different unsupervised machine learning algorithms\n",
    "    * figure out how many clusters we want to keep\n",
    "    * inspect the results of the clustering algorithm\n",
    "\n",
    "**Week 8 - Visualizing the results**\n",
    "10. create visualizations to compare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setup\n",
    "# plot the graphs inline\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Papers/paper0.txt', './Papers/paper1.txt', './Papers/paper10.txt', './Papers/paper11.txt', './Papers/paper12.txt', './Papers/paper13.txt', './Papers/paper14.txt', './Papers/paper15.txt', './Papers/paper16.txt', './Papers/paper2.txt', './Papers/paper3.txt', './Papers/paper4.txt', './Papers/paper5.txt', './Papers/paper6.txt', './Papers/paper7.txt', './Papers/paper8.txt', './Papers/paper9.txt']\n"
     ]
    }
   ],
   "source": [
    "# 1) using glob, find all the text files in the \"Papers\" folder\n",
    "# Hint: refer to last week's notebook\n",
    "import glob\n",
    "files = glob.glob('./Papers/*.txt')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) get all the data from the text files into the \"documents\" list\n",
    "# P.S. make sure you use the 'utf-8' encoding\n",
    "documents = []\n",
    "for paper in files:\n",
    "        f= open(paper,\"r\", encoding = 'utf8')\n",
    "        f = f.read()\n",
    "        documents.append(f)\n",
    "    \n",
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n"
     ]
    }
   ],
   "source": [
    "# 3) print the first 1000 characters of the first document to see what it \n",
    "# looks like (we'll use this as a sanity check below)\n",
    "print(documents[0][0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50043\n",
      "39310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) only select the text that's between the first occurence of the \n",
    "# the word \"abstract\" and the last occurence of the word \"reference\"\n",
    "# Optional: print the length of the string before and after, as a \n",
    "# sanity check\n",
    "# HINT: https://stackoverflow.com/questions/14496006/finding-last-occurrence-of-substring-in-string-replacing-that\n",
    "# read more about rfind: https://www.tutorialspoint.com/python/string_rfind.htm\n",
    "\n",
    "d1 = documents[0]\n",
    "print(len(d1))\n",
    "body = re.findall(r'abstract(.*?)reference', d1, re.DOTALL)\n",
    "body = body[0]\n",
    "print(len(body))\n",
    "type(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 participants. the key\n",
      "dependent variable was performance on a post-reading\n",
      "comprehension assessment. our results suggest that the\n",
      "technology was successful in correcting comprehension deficits\n",
      "attributed to mind wandering (d = .47\n"
     ]
    }
   ],
   "source": [
    "bodies = []\n",
    "print(len(documents))\n",
    "for doc in documents:\n",
    "    start = doc.index('abstract\\n') + len('abstract')\n",
    "    end = doc.rfind('\\nreference')\n",
    "    bodies.append(doc[start:end])\n",
    "len(bodies)\n",
    "print(bodies[0][0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' mind wandering, defined as shifts in attention from task-related processing to task-unrelated thoughts, is a ubiquitous phenomenon that has a negative influence on performance and productivity in many contexts, including learning. we propose that next-generation learning technologies should have some mechanism to detect and respond to mind wandering in real-time. towards this end, we developed a technology that automatically detects mind wandering from eye-gaze during learning from instructional texts. when mind wandering is detected, the technology intervenes by posing just-in-time questions and encouraging re-reading as needed. after multiple rounds of iterative refinement, we summatively compared the technology to a yoked-control in an experiment with 104 participants. the key dependent variable was performance on a post-reading comprehension assessment. our results suggest that the technology was successful in correcting comprehension deficits attributed to mind wandering (d = .47'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) replace carriage returns (i.e., \"\\n\") with a white space\n",
    "# check that the result looks okay by printing the \n",
    "# first 1000 characters of the 1st doc:\n",
    "# body = body.replace('\\n', ' ')\n",
    "# print(body[0:1000])\n",
    "#using enumerate- i is the index, doc is the element\n",
    "for i, doc in enumerate(bodies):\n",
    "    bodies[i] = doc.replace('\\n', ' ')\n",
    "bodies[0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mind wandering  defined as shifts in attention from task related processing to task unrelated thoughts  is a ubiquitous phenomenon that has a negative influence on performance and productivity in many contexts  including learning  we propose that next generation learning technologies should have some mechanism to detect and respond to mind wandering in real time  towards this end  we developed a technology that automatically detects mind wandering from eye gaze during learning from instructional texts  when mind wandering is detected  the technology intervenes by posing just in time questions and encouraging re reading as needed  after multiple rounds of iterative refinement  we summatively compared the technology to a yoked control in an experiment with 104 participants  the key dependent variable was performance on a post reading comprehension assessment  our results suggest that the technology was successful in correcting comprehension deficits attributed to mind wandering  d    47\n"
     ]
    }
   ],
   "source": [
    "# 6) replace the punctation below by a white space\n",
    "# check that the result looks okay \n",
    "# (e.g., by print the first 1000 characters of the 1st doc)\n",
    "\n",
    "punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '−', '”', '“', '’']\n",
    "\n",
    "for punc in punctuation:\n",
    "    bodies[0] = bodies[0].replace(punc, ' ')\n",
    "print(bodies[0][0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mind wandering  defined as shifts in attention from task related processing to task unrelated thoughts  is a ubiquitous phenomenon that has a negative influence on performance and productivity in many contexts  including learning  we propose that next generation learning technologies should have some mechanism to detect and respond to mind wandering in real time  towards this end  we developed a technology that automatically detects mind wandering from eye gaze during learning from instructional texts  when mind wandering is detected  the technology intervenes by posing just in time questions and encouraging re reading as needed  after multiple rounds of iterative refinement  we summatively compared the technology to a yoked control in an experiment with     participants  the key dependent variable was performance on a post reading comprehension assessment  our results suggest that the technology was successful in correcting comprehension deficits attributed to mind wandering  d      \n"
     ]
    }
   ],
   "source": [
    "# 7) remove numbers by either a white space or the word \"number\"\n",
    "# again, print the first 1000 characters of the first document\n",
    "# to check that you're doing the right thing\n",
    "\n",
    "\n",
    "bodies[0] = re.sub(r'\\d', ' ', bodies[0])\n",
    "print(bodies[0][0:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mind wandering  defined shifts attention task related processing task unrelated thoughts  ubiquitous phenomenon negative influence performance productivity many contexts  including learning  we propose next generation learning technologies mechanism detect respond mind wandering real time  towards end  we developed technology automatically detects mind wandering eye gaze learning instructional texts  mind wandering detected  technology intervenes posing time questions encouraging re reading needed  multiple rounds iterative refinement  we summatively compared technology yoked control experiment     participants  key dependent variable performance post reading comprehension assessment  results suggest technology successful correcting comprehension deficits attributed mind wandering  d       sigma  specific conditions  thereby highlighting potential improve learning  attending attention    keywords mind wandering  gaze tracking  student modeling  attentionaware      introduction despite\n"
     ]
    }
   ],
   "source": [
    "# 8) Remove the stop words below from our documents\n",
    "# print the first 1000 characters of the first document\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we ', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', \n",
    "              'just', 'don', 'should', 'now']\n",
    "\n",
    "for word in stop_words:\n",
    "    word = ' '+word+' '\n",
    "    bodies[0] = bodies[0].replace(word, ' ')\n",
    "print(bodies[0][0:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mind wandering  defined shifts attention task related processing task unrelated thoughts  ubiquitous phenomenon negative influence performance productivity many contexts  including learning propose next generation learning technologies mechanism detect respond mind wandering real time  towards developed technology automatically detects mind wandering gaze learning instructional texts  mind wandering detected  technology intervenes posing time questions encouraging reading needed  multiple rounds iterative refinement summatively compared technology yoked control experiment     participants dependent variable performance post reading comprehension assessment  results suggest technology successful correcting comprehension deficits attributed mind wandering       sigma  specific conditions  thereby highlighting potential improve learning  attending attention    keywords mind wandering  gaze tracking  student modeling  attentionaware      introduction despite best efforts write clear engag\n"
     ]
    }
   ],
   "source": [
    "# 9) remove words with one and two characters (e.g., 'd', 'er', etc.)\n",
    "# print the first 1000 characters of the first document\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "bodies[0]= shortword.sub('', bodies[0])\n",
    "print(bodies[0][0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) package all of your work above into a function that cleans a given document\n",
    "\n",
    "##Setup\n",
    "# plot the graphs inline\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "##Define the function\n",
    "\n",
    "def clean_list_of_documents(documents):\n",
    "#empty list to add documents to:    \n",
    "    cleaned_docs = []\n",
    "    \n",
    "#define lists for removal (punctuation, stop words and 2-letter words):\n",
    "    punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "                   '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "                   '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "                   '_', '^', '`', '{', '}', '|', '~', '−', '”', '“', '’']\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we ', 'our', 'ours', \n",
    "                  'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "                  'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "                  'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "                  'they', 'them', 'their', 'theirs', 'themselves', \n",
    "                  'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "                  'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "                  'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "                  'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "                  'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "                  'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "                  'between', 'into', 'through', 'during', 'before', \n",
    "                  'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "                  'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "                  'further', 'then', 'once', 'here', 'there', 'when', \n",
    "                  'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "                  'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "                  'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "                  'too', 'very', 's', 't', 'can', 'will', \n",
    "                  'just', 'don', 'should', 'now']\n",
    "\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "\n",
    "    #select only the body of the text:    \n",
    "    for document in documents:\n",
    "        start = document.index('abstract\\n') + len('abstract')\n",
    "        end = document.rfind('\\nreference')\n",
    "        document = document[start:end]\n",
    "    #remove line breaks: \n",
    "        document = document.replace('\\n', ' ')\n",
    "    #remove punctuation\n",
    "        for punc in punctuation:\n",
    "            document = document.replace(punc, ' ')\n",
    "    #remove numbers using regex:\n",
    "        document = re.sub(r'\\d', ' ', document)\n",
    "    #remove stop words\n",
    "        for word in stop_words:\n",
    "            word = ' '+word+' '\n",
    "            document = document.replace(word, ' ')\n",
    "    #remove short (1- and 2- letter) words:\n",
    "        document= shortword.sub('', document)\n",
    "    #append the cleaned document to the list cleaned_docs\n",
    "        cleaned_docs.append(document)\n",
    "    #return the results of cleaned_docs\n",
    "    return cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      " mind wandering  defined shifts attention task related processing task unrelated thoughts  ubiquitous phenomenon negative influence performance productivity many contexts  including learning propose next generation learning technologies mechanism detect respond mind wandering real time  towards developed technology automatically detects mind wandering gaze learning instructional texts  mind wandering detected  technology intervenes posing time questions encouraging reading needed  multiple rounds iterative refinement summatively compared technology yoked control experiment     participants dependent variable performance post reading comprehension assessment  results suggest technology successful correcting comprehension deficits attributed mind wandering       sigma  specific conditions  thereby highlighting potential improve learning  attending attention    keywords mind wandering  gaze tracking  student modeling  attentionaware      introduction despite best efforts write clear engag\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# 11a) reimport your raw data using the code in 2)\n",
    "\n",
    "files = glob.glob('./Papers/*.txt')\n",
    "\n",
    "documents = []\n",
    "for paper in files:\n",
    "        f= open(paper,\"r\", encoding = 'utf8')\n",
    "        f = f.read()\n",
    "        documents.append(f)\n",
    "print(len(documents))\n",
    "    \n",
    "        \n",
    "# 11b) clean your files using the function above\n",
    "documents = clean_list_of_documents(documents)\n",
    "\n",
    "\n",
    "# 11c) print the first 1000 characters of the first document\n",
    "print(documents[0][0:1000])\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Build your list of vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of words (i.e., the vocabulary) is going to become the columns of your matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Describe why we need to figure out the vocabulary used in our corpus (refer back to Sherin's paper, and explain in your own words): \n",
    "\n",
    "We need a list of all the unique words that might be important to count. By counting these unique words in each paper/passage, we can identify patterns of what the texts are about. With the vocabularly we will be able to count the frequency of these words in each passage we identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5876\n"
     ]
    }
   ],
   "source": [
    "# 13) create a function that takes in a list of documents\n",
    "# and returns a set of unique words. Make sure that you\n",
    "# sort the list alphabetically before returning it. \n",
    "\n",
    "def get_vocabulary(documents):\n",
    "    voc = []\n",
    "    for document in documents:\n",
    "        for word in document.split():\n",
    "            if word not in voc:\n",
    "                voc.append(word)\n",
    "    voc.sort()\n",
    "    return voc\n",
    "\n",
    "\n",
    "# Then print the length of your vocabulary (it should be \n",
    "# around 5500 words)\n",
    "vocabulary = get_vocabulary(documents)\n",
    "print(len(vocabulary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14) what was the size of Sherin's vocabulary? \n",
    "647 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - transform your documents into 100-words chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2231\n",
      "['study', 'capturing', 'semantic', 'attributes', 'student', 'responses', 'vocabulary', 'learning', 'also', 'limitations', 'current', 'study', 'areas', 'future', 'work', 'first', 'expanding', 'scope', 'analysis', 'full', 'experimental', 'conditions', 'used', 'study', 'reveal', 'complex', 'interactions', 'conditions', 'students', 'short', 'longterm', 'learning', 'second', 'study', 'used', 'fixed', 'threshold', 'investigating', 'false', 'prediction', 'results', 'however', 'optimal', 'threshold', 'participant', 'group', 'prediction', 'model', 'could', 'selected', 'especially', 'different', 'false', 'positive', 'negative', 'patterns', 'observed', 'different', 'groups', 'students', 'lastly', 'study', 'collected', 'data', 'single', 'vocabulary', 'tutoring', 'system', 'used', 'classroom', 'setting', 'applying', 'proposed', 'method', 'data', 'collected', 'classroom', 'setting', 'vocabulary', 'learning', 'system', 'would', 'useful', 'show', 'generalization', 'suggested', 'method', 'acknowledgments', 'research', 'reported', 'supported', 'institute', 'education', 'sciences', 'department', 'education', 'grant', 'university', 'michigan', 'opinions']\n"
     ]
    }
   ],
   "source": [
    "# 15) create a function that takes in a list of documents\n",
    "# and returns a list of 100-words chunk \n",
    "# (with a 25 words overlap between them)\n",
    "# Optional: add two arguments, one for the number of words\n",
    "# in each chunk, and one for the overlap size\n",
    "# Advice: combining all the documents into one giant string\n",
    "# and splitting it into separate words will make your life easier!\n",
    "\n",
    "def text_segments(text, overlap, length):\n",
    "    text = ' '.join(text)\n",
    "    text = text.split()\n",
    "    segments = []\n",
    "    for i in range(0,len(text)-100,overlap):\n",
    "        segments.append(text[i:i+length])\n",
    "        i += 25\n",
    "    return segments\n",
    "\n",
    "segments = text_segments(documents,25,100)\n",
    "print(len(segments))\n",
    "\n",
    "#We have to limit the end segments so they are all 100 words- otherwise we end up with the final 100 words split\n",
    "#into 25-word chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 16) create a for loop to double check that each chunk has \n",
    "# a length of 100\n",
    "# Optional: use assert to do this check\n",
    "def hundreds_true(text):\n",
    "    for i in range(0,len(text)):\n",
    "        assert len(seg) == 100\n",
    "    return 'true'\n",
    "hundreds_true(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mind', 'wandering', 'defined', 'shifts', 'attention', 'task', 'related', 'processing', 'task', 'unrelated', 'thoughts', 'ubiquitous', 'phenomenon', 'negative', 'influence', 'performance', 'productivity', 'many', 'contexts', 'including', 'learning', 'propose', 'next', 'generation', 'learning', 'technologies', 'mechanism', 'detect', 'respond', 'mind', 'wandering', 'real', 'time', 'towards', 'developed', 'technology', 'automatically', 'detects', 'mind', 'wandering', 'gaze', 'learning', 'instructional', 'texts', 'mind', 'wandering', 'detected', 'technology', 'intervenes', 'posing', 'time', 'questions', 'encouraging', 'reading', 'needed', 'multiple', 'rounds', 'iterative', 'refinement', 'summatively', 'compared', 'technology', 'yoked', 'control', 'experiment', 'participants', 'dependent', 'variable', 'performance', 'post', 'reading', 'comprehension', 'assessment', 'results', 'suggest', 'technology', 'successful', 'correcting', 'comprehension', 'deficits', 'attributed', 'mind', 'wandering', 'sigma', 'specific', 'conditions', 'thereby', 'highlighting', 'potential', 'improve', 'learning', 'attending', 'attention', 'keywords', 'mind', 'wandering', 'gaze', 'tracking', 'student', 'modeling']\n"
     ]
    }
   ],
   "source": [
    "# 17) print the first chunk, and compare it to the original text.\n",
    "# does that match what Sherin describes in his paper?\n",
    "print(segments[0])\n",
    "\n",
    "Abstract from paper 1:\n",
    "# Mind wandering, defined as shifts in attention from task-related\n",
    "# processing to task-unrelated thoughts, is a ubiquitous\n",
    "# phenomenon that has a negative influence on performance and\n",
    "# productivity in many contexts, including learning. We propose\n",
    "# that next-generation learning technologies should have some\n",
    "# mechanism to detect and respond to mind wandering in real-time.\n",
    "# Towards this end, we developed a technology that automatically\n",
    "# detects mind wandering from eye-gaze during learning from\n",
    "# instructional texts. When mind wandering is detected, the\n",
    "# technology intervenes by posing just-in-time questions and\n",
    "# encouraging re-reading as needed. After multiple rounds of\n",
    "# iterative refinement, we summatively compared the technology to\n",
    "# a yoked-control in an experiment with 104 participants. The key\n",
    "# dependent variable was performance on a post-reading\n",
    "# comprehension assessment. Our results suggest that the\n",
    "# technology was successful in correcting comprehension deficits\n",
    "# attributed to mind wandering (d = .47 sigma) under specific\n",
    "# conditions, thereby highlighting the potential to improve learning\n",
    "# by “attending to attention.”\n",
    "\n",
    "##Yep, this looks right to me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18) how many chunks did Sherin have? What does a chunk become in the next step of our topic modeling algorithm? \n",
    "Sherin had 794 segments of text. These segments were converted from a list of words, to a count of the vocabulary. So in the segment printed in question 1, insread of 100 words in a list, we might have a dictionary that held a key for each word and a value that was the count of how often that word appeared in this segment (e.g. 'mind':4). Then this is converted to a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19) what are some other preprocessing steps we could do to improve the quality of the text data? Mention at least 2.\n",
    "I don't think we should have made these segments cross papers. We probably should have made 100-word segments bounded by each paper so we can compare the papers to each other (like Sherin does for different students)\n",
    "\n",
    "I think we might have wanted to keep compound words, so not removing all hyphens from the text. In the first paper \"mind-wandering\" is meaningfully different from \"mind\" and \"wandering\" separately.\n",
    "\n",
    "We might also want to remove the keywords that appear in between the abstract and introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20) in your own words, describe the next steps of the data modeling algorithms (listed below):\n",
    "We will take the segments and convert them into vocabulary counts, so all the segments will have a matrix of the same list of words and the unique frequency of each to that segment.\n",
    "\n",
    "We will \"weight\" the words, actually log-transforming them so that they have a more linear relationship and giving less weight to those words that have very high frequencies\n",
    "\n",
    "We will create vectors out of these: which will have two properties, an angle and a length.\n",
    "\n",
    "We will standardize the lengths to 1 so the vectors only have one property.\n",
    "\n",
    "We will convert these into deviation vectors that tell us not the unique angle of the vector but its deviation from the average.\n",
    "\n",
    "We will cluster the vectors by comparing them to each other- vectors with similar averages will iteratively be clustered together. We then have to choose the best cluster scheme.\n",
    "\n",
    "We will visualize the results- how do the segments compare, and can we extract any meaning about what the EDM conference is about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Vector and Matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Weight word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Matrix normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Deviation Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step - Putting it all together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
